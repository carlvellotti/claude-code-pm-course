---
title: '2.2: Analyze Data'
description: 'Make data-driven product decisions with AI-assisted analysis'
---

## Reference Guide

**Last Updated:** October 16, 2025
**Time to Complete:** 60-75 minutes (interactive module)
**Prerequisites:** Module 2.1 (Writing PRDs), basic understanding of CSV files and product metrics

---

## üìñ Overview

This module teaches the complete PM workflow for data-driven feature development: discovering problems through data analysis, estimating business impact before building, and analyzing experiment results to make ship/kill decisions. You'll learn to use Claude Code as your data analysis partner, processing CSV files from analytics tools, building ROI models, and running statistical analyses on A/B tests.

**Key takeaway:** Never stop at topline metrics‚Äîalways segment by your target customer, check quality over quantity, and look for leading indicators that predict long-term success.

---

## üéØ The Three-Phase Workflow

### Phase 1: Discovery - Find Problems with Data

Use quantitative and qualitative data to identify WHERE users are dropping off and WHY.

**What you'll do:**
- Analyze funnel data (CSV exports from Mixpanel, Amplitude, etc.)
- Process survey responses to understand user pain points
- Cross-reference quantitative drop-offs with qualitative feedback
- Synthesize findings into a problem analysis document

**Example scenario:** Your activation rate is stuck at 45% for 6 months. Leadership wants answers. Instead of guessing, you analyze the funnel and discover 60% of users drop between "create task" and "complete task." Survey data reveals they're overwhelmed by the blank canvas and need examples.

**Deliverable:** `problem-analysis.md` documenting the problem, supporting evidence, and proposed solution.

---

### Phase 2: Impact Estimation - Is It Worth Building?

Build ROI models to justify engineering investment before writing a single line of code.

**The framework:**
```
Impact = Users Affected √ó Current Action Rate √ó Expected Lift √ó Value per Action
```

**What you'll do:**
- Calculate baseline metrics from usage data
- Estimate lift based on user research, competitor benchmarks, and historical data
- Model business impact (revenue, activation, retention)
- Create three scenarios (pessimistic/realistic/optimistic) to acknowledge uncertainty
- Calculate ROI over 1 year and 3 years

**Example scenario:** Your proposed Guided Onboarding feature will cost $100k (4 eng-months). You estimate it will improve activation from 45% ‚Üí 58% for 70% of users, generating $39k ARR in year 1 and $943k in LTV over 3 years. ROI: 9.4x over 3 years.

**Deliverables:**
- `impact-estimate.md` with baseline, projections, and business impact
- `roi-scenarios.md` with pessimistic/realistic/optimistic cases

---

### Phase 3: Experiment Analysis - Did It Work?

Analyze A/B test results beyond topline metrics to find the real story.

**What you'll do:**
- Calculate topline activation rates (treatment vs control)
- Run statistical significance tests (p-values, confidence intervals)
- Segment by customer type (small teams vs enterprise)
- Check quality metrics (retention, engagement, not just quantity)
- Analyze leading indicators (feature adoption, viral metrics)
- Write experiment readout with ship/iterate/kill recommendation

**Example scenario:** Your topline shows only 45% ‚Üí 48% activation (+2.6pp, barely significant). Looks disappointing! But segmentation reveals +11.4pp for small teams (huge win!) and -3.5pp for enterprise (negative effect). Quality metrics show 78% vs 60% retention among activated users. Recommendation: Ship to small teams, exclude enterprise.

**Deliverable:** `experiment-readout.md` with executive summary, segment analysis, quality metrics, and clear recommendation.

---

## üíº Using Claude Code for Data Analysis

### What Claude Code Can Do

**Read and process CSV files:**
```
"Read activation-funnel-q4.csv and calculate drop-off rates at each step"
```

Claude will:
- Read the CSV file directly
- Parse and analyze the data
- Calculate metrics (percentages, drop-offs, averages)
- Present results in clean markdown tables

**Analyze thousands of rows instantly:**
```
"Analyze the 8,000 rows in onboarding-experiment-results.csv and segment activation rates by company size"
```

Claude will:
- Process all 8,000 rows
- Group by company_size (5-20, 21-99, 100+)
- Calculate activation rates for each segment
- Run statistical tests
- Present formatted results with insights

**Build ROI models:**
```
"Build an impact estimation model using the framework in impact-estimation-framework.md"
```

Claude will:
- Read your usage data and calculate baselines
- Apply the framework (Users √ó Rate √ó Lift √ó Value)
- Project business impact (revenue, users, metrics)
- Create scenario analyses
- Generate polished documents

**Run statistical analyses:**
```
"Calculate statistical significance between control and treatment groups"
```

Claude will:
- Calculate p-values
- Compute confidence intervals
- Determine if results are statistically significant
- Explain what the statistics mean in plain English

---

## üìä The Impact Estimation Framework

### The Formula

```
Impact = Users Affected √ó Current Action Rate √ó Expected Lift √ó Value per Action
```

### Component Breakdown

**1. Users Affected**
- How many users will see this feature?
- Account for gradual rollout (not always 100%)
- Consider segment targeting (e.g., only small teams)

**Example:** 5,000 signups/month √ó 70% see guided onboarding = 3,500 users affected

**2. Current Action Rate**
- What % currently take the desired action?
- Get from analytics tool (Mixpanel, Amplitude)

**Example:** 45% activation rate (2,025/4,500 complete first task)

**3. Expected Lift**
- How much will the feature improve the rate?
- **This is the hardest part!** Sources:
  - Similar features you've shipped
  - Competitor benchmarks
  - User research (60% drop due to X, fix X ‚Üí recover 50% of that)
  - Expert judgment from eng/design/PM

**Example:** Survey shows 60% drop due to "need examples." Fix that, conservatively recover 30% of drop ‚Üí estimate 13pp lift (45% ‚Üí 58%)

**4. Value per Action**
- What's each incremental action worth?
- For activation: LTV √ó conversion rate
- For retention: extended LTV
- For viral: invite acceptance √ó activation √ó conversion √ó LTV

**Example:** Activated user ‚Üí 60% convert √ó $12/mo √ó 24 months = $172.80 LTV per activation

### Three-Scenario Approach

**Always model uncertainty:**

**Pessimistic (20th percentile):**
- Lower adoption (30% vs 70%)
- Smaller lift (45% ‚Üí 50% vs 58%)
- Minimum expected impact

**Realistic (50th percentile):**
- Expected adoption (70%)
- Conservative lift estimate (45% ‚Üí 58%)
- Most likely case

**Optimistic (80th percentile):**
- High adoption (90%)
- Strong lift plus secondary effects (45% ‚Üí 62% + retention boost)
- Best case scenario

**Present all three to leadership** so they understand the range of outcomes and can make informed bets.

---

## üî¨ Experiment Analysis Best Practices

### Never Stop at Topline Metrics

**The trap:** You run an A/B test. Topline shows modest results (45% ‚Üí 48%, p=0.04). You consider killing it.

**The mistake:** You haven't segmented yet!

**The lesson:** Topline metrics can hide segment wins. Always segment by your target customer before making decisions.

### Always Segment by Target Customer

**Why it matters:**
- Features often work differently for different user types
- Your target market (e.g., small teams) might see huge wins
- Non-target segments (e.g., enterprise) might have negative effects
- Topline averages these out, masking the truth

**How to do it:**
```
"Segment the experiment results by company_size and calculate activation rates for each segment"
```

Claude will break down results by segment and show you where the feature really works.

### Check Quality Over Quantity

**Activation rate tells you HOW MANY users activated.**

**Retention tells you if those are GOOD activations.**

**Always check:**
- Week 1 retention among activated users
- Engagement metrics (tasks completed, features used)
- Long-term retention (30-day, 60-day)

**Why:** You want better users, not just more users. 78% retention with 48% activation is way better than 60% retention with 50% activation.

### Look for Leading Indicators

**Leading indicators predict future success:**

- **Feature adoption:** Do users engage with the new feature?
- **Viral metrics:** Do users invite teammates?
- **Template usage:** Do users adopt productivity shortcuts?
- **Depth of engagement:** Do users use advanced features?

**Example:** Invite rate 35% vs 12% means 2.9x more viral growth, predicting higher LTV and lower CAC.

### Statistical Significance Matters

**p < 0.05:** Statistically significant (less than 5% chance this is random noise)

**p > 0.05:** Not statistically significant (could be random)

**Confidence intervals:** Show the range of plausible effect sizes

**Example:**
- Lift: +2.6pp
- p-value: 0.04 (barely significant)
- 95% CI: [0.1%, 5.1%] (wide range - could be small or moderate)

**Interpretation:** Barely significant with wide uncertainty ‚Üí dig deeper before deciding!

---

## üí° Real-World PM Scenarios

### Scenario 1: Stuck Activation Rate

**Situation:** Your activation has plateaued at 45% for 6 months. Leadership is asking what you're doing about it.

**How Claude Code helps:**
1. **Analyze funnel data:** "Read activation-funnel-q4.csv and find the biggest drop-off"
2. **Process surveys:** "Analyze user-survey-responses.csv and extract top complaints"
3. **Cross-reference:** "The 60% drop-off at 'task completion' correlates with survey feedback about 'need examples'"
4. **Synthesize:** "Create a problem analysis document with quantitative and qualitative evidence"

**Outcome:** Clear problem statement backed by data, proposed solution, and stakeholder alignment document ready to share.

---

### Scenario 2: Justifying a Major Feature

**Situation:** You want to build Guided Onboarding ($100k, 4 eng-months). Engineering is skeptical, leadership wants ROI projections.

**How Claude Code helps:**
1. **Show framework:** "Read impact-estimation-framework.md and explain the formula"
2. **Calculate baseline:** "Analyze taskflow-usage-data-q4.csv to calculate current activation rate and time-to-value"
3. **Estimate lift:** "Based on the survey data showing 60% need examples, estimate conservative lift"
4. **Model impact:** "Build complete ROI model with baseline, projections, and business impact"
5. **Create scenarios:** "Generate pessimistic, realistic, and optimistic scenarios"

**Outcome:** Comprehensive impact estimate showing 9.4x ROI over 3 years, with scenario analysis showing even pessimistic case returns 2.6x. Engineering and leadership approve the build.

---

### Scenario 3: Analyzing Experiment Results

**Situation:** Your Guided Onboarding experiment finished. Topline shows only 45% ‚Üí 48% (+2.6pp). Team is disappointed and considering killing it.

**How Claude Code helps:**
1. **Check topline:** "Calculate overall activation rates for control and treatment"
2. **Segment:** "Segment results by company_size - what do small teams show?"
3. **Quality metrics:** "Among activated users, calculate week 1 retention for both cohorts"
4. **Leading indicators:** "Compare template usage and invite rates between cohorts"
5. **Synthesize:** "Create experiment readout with recommendation"

**Outcome:** Segmentation reveals +11.4pp for small teams (your target!), retention jumps 60% ‚Üí 78%, template usage 3.2x higher. Clear recommendation: Ship to small teams, exclude enterprise. What looked like a failure is actually a huge win.

---

## üìÅ Working with CSV Data Files

### Data Sources PMs Use

**Analytics platforms:**
- Mixpanel, Amplitude ‚Üí Export usage events, funnels
- Optimizely, LaunchDarkly ‚Üí Export A/B test results
- Qualtrics, SurveyMonkey ‚Üí Export survey responses
- Google Analytics ‚Üí Export traffic/conversion data

**File formats:**
- CSV (most common)
- TSV (tab-separated values)
- JSON (from APIs)

**Claude Code can read all of these directly!**

### Viewing CSV Files

**Note:** CSV files won't render nicely in Obsidian (they're not markdown).

**Options for viewing:**
- **Excel or Google Sheets:** Best for exploring data visually
- **VS Code:** Good for viewing raw structure
- **Numbers (Mac):** Native Mac spreadsheet app
- **Let Claude read it:** Claude will format data in clean markdown tables for you

**Recommended approach:** Let Claude read and analyze the CSV, presenting results in formatted tables. View raw CSV only if you need to verify specific data points.

### Sample CSV Structure

**activation-funnel-q4.csv:**
```csv
step,users_entered,users_completed,completion_rate,median_time_to_complete
Signup,10000,10000,1.0,0
First Task Created,10000,7200,0.72,18
First Task Completed,7200,2880,0.40,45
Invite Sent,2880,1440,0.50,24
```

**onboarding-experiment-results.csv:**
```csv
user_id,cohort,company_size,completed_first_task,invited_teammate,tasks_completed_week_1
control_user_0001,control,5-20,True,False,4
control_user_0002,control,5-20,False,False,0
treatment_user_0001,treatment,5-20,True,True,8
```

Claude reads these and presents them as:

| Step | Users Entered | Completion Rate | Drop-off |
|------|---------------|-----------------|----------|
| Signup | 10,000 | 100% | - |
| First Task Created | 7,200 | 72% | 28% |
| First Task Completed | 2,880 | 40% | 60% ‚Üê Biggest drop |

---

## üéØ Best Practices

### Do:

- ‚úÖ **Always validate hypotheses with data** - Don't guess, analyze
- ‚úÖ **Create three scenarios for every estimate** - Acknowledge uncertainty
- ‚úÖ **Segment by target customer** - Topline can hide segment wins
- ‚úÖ **Check quality metrics** - Retention matters more than activation count
- ‚úÖ **Look for leading indicators** - Predict long-term success early
- ‚úÖ **Document your assumptions** - Make impact estimates transparent
- ‚úÖ **Cross-reference quant + qual data** - Funnel + surveys tell full story
- ‚úÖ **Present full experiment context** - Not just "it worked" but WHY and FOR WHOM

### Don't:

- ‚ùå **Don't stop at topline metrics** - Always dig deeper with segmentation
- ‚ùå **Don't use single-point estimates** - Use ranges and scenarios
- ‚ùå **Don't assume 100% adoption** - Account for gradual rollout
- ‚ùå **Don't ignore negative segments** - If enterprise performs poorly, exclude them
- ‚ùå **Don't kill experiments too early** - Check segments and quality first
- ‚ùå **Don't forget statistical significance** - p-values and confidence intervals matter
- ‚ùå **Don't over-optimize lift estimates** - Be conservative, under-promise
- ‚ùå **Don't skip the "why"** - Always explain the reasoning behind numbers

### Pro Tips:

1. **Use historical data when available** - Your past experiments are the best predictor of future lift, not competitor benchmarks

2. **Front-load the disappointing news** - When presenting experiment results, show the modest topline first, then reveal the segment wins. This teaches stakeholders to always dig deeper.

3. **Build a lift estimate library** - Track every feature's estimated vs actual lift. After 5-10 features, you'll get much better at estimating.

4. **Automate your analysis scripts** - Save the prompts you use for funnel analysis, segmentation, and ROI modeling. Reuse them across features.

5. **Show your work** - When presenting impact estimates, include the full calculation breakdown. Transparency builds trust.

6. **Create reusable templates** - The documents you create (problem-analysis.md, impact-estimate.md, experiment-readout.md) become templates for future features.

---

## üêõ Troubleshooting

### "Claude can't read my CSV file"

**Likely cause:** File path is incorrect or file isn't in the project directory.

**Fix:**
1. Check file path with `ls` or file browser
2. Use correct relative path (e.g., `data/experiment-results.csv`)
3. Verify file is in accessible directory (not hidden folder)
4. Make sure file extension is `.csv` not `.CSV` (case-sensitive on some systems)

---

### "Results don't match what I see in Excel"

**Likely cause:** Different calculations or data interpretation.

**Fix:**
1. Ask Claude to show the calculation step-by-step
2. Verify Claude is using the correct columns
3. Check for data type issues (True/False vs 1/0)
4. Ask Claude: "Explain how you calculated activation rate from this CSV"
5. Cross-check a few rows manually

---

### "Statistical significance seems wrong"

**Likely cause:** Sample size too small, or misunderstanding of p-values.

**Fix:**
1. Check sample size: Need ~400+ users per cohort for reliable significance testing
2. Verify Claude used correct statistical test (proportion test for activation rates)
3. Remember: `p<0.05` means "less than 5% chance this is random" not "5% error"
4. Wide confidence intervals = high uncertainty, even if `p<0.05`
5. Ask Claude: "Explain the p-value and confidence interval in plain English"

---

### "My lift estimate seems too high/low"

**Likely cause:** Incorrect application of the framework or unrealistic assumptions.

**Fix:**
1. Review each component: Users Affected, Current Rate, Expected Lift, Value per Action
2. Check if "Expected Lift" is based on data or wishful thinking
3. Use conservative estimates - better to under-promise
4. Compare to historical lifts from similar features
5. Create three scenarios to bound the estimate
6. Ask Claude: "What assumptions went into this lift estimate? Are they conservative?"

---

### "Segment analysis shows conflicting results"

**Likely cause:** This is actually normal! Features work differently for different users.

**Fix:**
1. This isn't a bug - it's an insight!
2. Features often win for target segment and lose for others
3. Example: Guided onboarding helps small teams (+11pp) but hurts enterprise (-3pp)
4. Solution: Ship to winning segment, exclude losing segment
5. Build different solutions for different segments
6. Document this learning for future features

---

## üìö Community Resources

### Official Documentation

- [Claude Code Documentation](https://docs.claude.com/claude-code) - Official getting started guide
- [Working with Files](https://docs.claude.com/claude-code/guides/working-with-files) - How Claude reads and processes files
- [Data Analysis with Claude](https://docs.claude.com/claude-code/examples/data-analysis) - Official examples of data analysis workflows

### Impact Estimation Resources

- [Reforge: Growth Models](https://www.reforge.com/blog/growth-models) - Framework for building growth models and estimating impact
- [Amplitude: Product Analytics Guide](https://amplitude.com/guides/product-analytics) - How to use product analytics for decision-making
- [Mixpanel: A/B Testing Guide](https://mixpanel.com/topics/ab-testing-guide/) - Statistical concepts for experiment analysis

### Statistical Analysis Tools

- **For deeper stats:** [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) - Python library for statistical tests (Claude can use this!)
- **For visualization:** Ask Claude to create markdown tables and ASCII charts
- **For significance calculators:** [Evan's Awesome A/B Tools](https://www.evanmiller.org/ab-testing/) - Quick significance calculators

### PM Impact Estimation Articles

- [How to Estimate Impact Before Building](https://www.lennysnewsletter.com/p/how-to-estimate-impact) - Lenny Rachitsky's guide
- [The ROI of Product Features](https://www.productplan.com/learn/calculate-roi-product-feature/) - ProductPlan framework
- [Data-Driven Product Management](https://www.intercom.com/blog/data-driven-product-management/) - Intercom's approach

### Example Datasets for Practice

- [Kaggle: Product Analytics Datasets](https://www.kaggle.com/datasets?search=product+analytics) - Public datasets to practice with
- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) - Academic datasets including user behavior data

---

## üöÄ What's Next?

### After Module 2.2

You now understand:
- ‚úÖ How to analyze funnel and survey data to discover product problems
- ‚úÖ The impact estimation framework (Users √ó Rate √ó Lift √ó Value)
- ‚úÖ How to build ROI models with scenario analyses
- ‚úÖ How to analyze A/B tests beyond topline metrics
- ‚úÖ When to segment, check quality metrics, and look for leading indicators
- ‚úÖ How to use Claude Code as your data analysis partner

**Ready for Module 2.3: Competitive Research & Strategic Analysis?**

In the next module, you'll learn to:
- Conduct rapid competitive research with parallel agents
- Apply strategic frameworks (SWOT, Porter's Five Forces, Blue Ocean)
- Analyze market opportunities and threats
- Create competitive positioning documents
- Use Claude Code for strategic thinking, not just execution

**[Go to Module 2.3 Reference ‚Üí](../2.3-competitive-research/REFERENCE_GUIDE.md)**

Or, if you're doing the interactive track:
```
Type: /start-2-3
```

---

## üìñ Key Terms Reference

**Activation Rate:** Percentage of signups who complete a key action (e.g., complete first task)

**A/B Test:** Experiment comparing two versions (control vs treatment) to measure impact

**Confidence Interval:** Range of plausible values for the true effect size (e.g., 95% CI: [0.1%, 5.1%])

**Control Group:** Users who see the existing experience (baseline for comparison)

**Funnel Analysis:** Tracking users through sequential steps to identify drop-off points

**Impact Estimation:** Projecting business value of a feature before building it

**Leading Indicator:** Early metric that predicts future success (e.g., invite rate predicts retention)

**Lift:** Improvement in metric (e.g., activation 45% ‚Üí 58% = +13pp lift)

**LTV (Lifetime Value):** Total revenue from a customer over their entire relationship

**p-value:** Probability the observed effect is due to random chance (`p<0.05` = statistically significant)

**ROI (Return on Investment):** Revenue or value generated divided by cost (e.g., 9.4x ROI)

**Segment:** Subset of users grouped by shared characteristic (e.g., company size, role)

**Statistical Significance:** Result is unlikely to be due to random chance (typically `p<0.05`)

**Topline Metric:** Overall average metric before segmentation

**Treatment Group:** Users who see the new experience being tested

---

**Remember:** The best PMs use data to discover problems, estimate impact before building, and analyze results to learn and iterate. Claude Code makes this workflow 10x faster by processing data, running analyses, and generating polished documents‚Äîall while you focus on the strategic decisions.
